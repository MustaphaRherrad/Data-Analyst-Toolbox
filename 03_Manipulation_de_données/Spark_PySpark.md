**Apache Spark** est un framework open-source pour le **traitement de donnÃ©es massives**. **PySpark** est son interface Python, permettant dâ€™utiliser Spark avec des **DataFrames, SQL et Machine Learning**.

---

## âœ… **1. Installation de PySpark**  

### **ðŸ”¹ Installer PySpark avec pip**
Si PySpark nâ€™est pas installÃ©, ajoute-le via **pip** :  
```bash
pip install pyspark
```

### **ðŸ”¹ VÃ©rifier lâ€™installation**  
Dans Python, vÃ©rifie que PySpark fonctionne :  
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TestSpark").getOrCreate()
print(spark.version)  # Affiche la version installÃ©e
```

---

## ðŸ“¦ **2. CrÃ©ation dâ€™une session Spark**  

### **ðŸ”¹ DÃ©marrer Spark**
```python
spark = SparkSession.builder.appName("MonAnalyse").getOrCreate()
```

---

## ðŸ“Š **3. Manipulation des DataFrames avec PySpark**  

### **ðŸ”¹ Charger un fichier CSV**
```python
df = spark.read.csv("data.csv", header=True, inferSchema=True)
df.show()
```

### **ðŸ”¹ Afficher la structure des donnÃ©es**
```python
df.printSchema()
df.describe().show()
```

### **ðŸ”¹ SÃ©lectionner des colonnes**
```python
df.select("Nom", "Ã‚ge").show()
```

### **ðŸ”¹ Filtrer les donnÃ©es**
```python
df_filtre = df.filter(df["Ã‚ge"] > 30)
df_filtre.show()
```

---

## ðŸ”„ **4. Groupement et agrÃ©gation**  

### **ðŸ”¹ Calculer la moyenne par groupe**
```python
df_grouped = df.groupBy("Ville").agg({"Salaire": "mean"})
df_grouped.show()
```

### **ðŸ”¹ Trier les rÃ©sultats**
```python
df.orderBy(df["Salaire"].desc()).show()
```

---

## ðŸš€ **5. ExÃ©cution de requÃªtes SQL**  

### **ðŸ”¹ CrÃ©er une table temporaire**
```python
df.createOrReplaceTempView("employes")
```

### **ðŸ”¹ ExÃ©cuter une requÃªte SQL**
```python
df_sql = spark.sql("SELECT Nom, Age FROM employes WHERE Age > 30")
df_sql.show()
```

---

## ðŸ“ˆ **6. Manipulation avancÃ©e avec Spark MLlib**  

### **ðŸ”¹ Appliquer une transformation avec VectorAssembler**
```python
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=["Age", "Salaire"], outputCol="features")
df_vect = assembler.transform(df)
df_vect.show()
```

### **ðŸ”¹ Appliquer un modÃ¨le de rÃ©gression**
```python
from pyspark.ml.regression import LinearRegression

lr = LinearRegression(featuresCol="features", labelCol="Salaire")
model = lr.fit(df_vect)
predictions = model.transform(df_vect)
predictions.show()
```

---

## ðŸ”¥ **RÃ©sumÃ©**
| **FonctionnalitÃ©** | **Commande** |
|----------------|------------|
| **Installation** | `pip install pyspark` |
| **CrÃ©er une session** | `SparkSession.builder.appName("Nom").getOrCreate()` |
| **Charger CSV** | `spark.read.csv("data.csv", header=True)` |
| **Filtrer les donnÃ©es** | `df.filter(df["Ã‚ge"] > 30)` |
| **RequÃªtes SQL** | `spark.sql("SELECT * FROM table")` |
| **Machine Learning** | `from pyspark.ml.regression import LinearRegression` |

---

Avec **PySpark**, tu peux manipuler **dâ€™Ã©normes volumes de donnÃ©es** et faire du **Big Data Analytics** ! 
