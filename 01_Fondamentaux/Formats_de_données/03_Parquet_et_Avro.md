Les formats **Parquet** et **Avro** sont largement utilisÃ©s en **Big Data** et en **stockage optimisÃ©**.  

---

## ğŸ“¦ **Format Parquet**  

### **ğŸ”¹ Description**  
**Parquet** est un format de fichier **colonnaire**, conÃ§u pour optimiser les performances lors de la lecture de donnÃ©es volumineuses. Il est particuliÃ¨rement utilisÃ© dans les **architectures Big Data** avec **Apache Spark, Hive et Hadoop**.  

### **ğŸ“Œ CaractÃ©ristiques**  
âœ”ï¸ **Stockage colonnaire** â†’ Permet des requÃªtes rapides sur certaines colonnes sans charger tout le fichier.  
âœ”ï¸ **Compression efficace** â†’ Utilise des algorithmes comme Snappy, Gzip ou LZO pour rÃ©duire lâ€™espace de stockage.  
âœ”ï¸ **InteropÃ©rabilitÃ©** â†’ Compatible avec de nombreux outils **Big Data**.  
âœ”ï¸ **AdaptÃ© aux bases de donnÃ©es distribuÃ©es** â†’ AccÃ©lÃ¨re les traitements en environnement distribuÃ©.  

### **ğŸ“œ Exemple d'utilisation en Python avec Pandas**  
```python
import pandas as pd

# CrÃ©ation dâ€™un DataFrame
data = {"Nom": ["Alice", "Bob", "Charlie"], "Ã‚ge": [25, 30, 28], "Ville": ["Paris", "Lyon", "Marseille"]}
df = pd.DataFrame(data)

# Enregistrer au format Parquet
df.to_parquet("data.parquet", engine="pyarrow", compression="snappy")

# Charger un fichier Parquet
df_parquet = pd.read_parquet("data.parquet")
print(df_parquet)
```

### **ğŸ“Œ Utilisation avec PySpark**  
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ParquetExample").getOrCreate()

# Lire un fichier Parquet
df = spark.read.parquet("data.parquet")
df.show()

# Ã‰crire un DataFrame en Parquet
df.write.parquet("output.parquet")
```

---

## ğŸ—ï¸ **Format Avro**  

### **ğŸ”¹ Description**  
**Avro** est un format **binaire** utilisÃ© pour le **stockage et le transfert de donnÃ©es**. Contrairement Ã  **Parquet**, il stocke les donnÃ©es **en ligne** (row-based), ce qui le rend plus adaptÃ© aux **transactions rapides** et Ã  lâ€™envoi de **donnÃ©es sÃ©rialisÃ©es**.  

### **ğŸ“Œ CaractÃ©ristiques**  
âœ”ï¸ **Format binaire compact** â†’ RÃ©duit la taille des fichiers.  
âœ”ï¸ **Inclut le schÃ©ma** â†’ Permet une validation stricte des donnÃ©es.  
âœ”ï¸ **OptimisÃ© pour la sÃ©rialisation/dÃ©sÃ©rialisation** â†’ IdÃ©al pour **Kafka et le streaming**.  
âœ”ï¸ **Compatible avec Java, Python et Hadoop** â†’ Sâ€™intÃ¨gre bien aux systÃ¨mes distribuÃ©s.  

### **ğŸ“œ Exemple d'utilisation en Python avec Fastavro**  
```python
import fastavro
import json

# DÃ©finir un schÃ©ma Avro
schema = {
    "type": "record",
    "name": "Employe",
    "fields": [
        {"name": "nom", "type": "string"},
        {"name": "age", "type": "int"},
        {"name": "ville", "type": "string"}
    ]
}

# DonnÃ©es Ã  enregistrer
data = [{"nom": "Alice", "age": 25, "ville": "Paris"}, {"nom": "Bob", "age": 30, "ville": "Lyon"}]

# Enregistrer au format Avro
with open("data.avro", "wb") as out_file:
    fastavro.writer(out_file, schema, data)

# Lire un fichier Avro
with open("data.avro", "rb") as in_file:
    reader = fastavro.reader(in_file)
    for record in reader:
        print(record)
```

---

## ğŸ”¥ **Comparaison : Parquet vs Avro**
| **Format**  | **Stockage** | **Utilisation principale** | **Avantages** |
|-------------|-------------|----------------------------|---------------|
| **Parquet** | Colonnaire  | Analytics, Big Data       | RequÃªtes rapides, compression efficace |
| **Avro**    | Ligne       | Streaming, transactions   | SÃ©rialisation rapide, intÃ©gration avec Kafka |

---

### **ğŸ¯ Quand choisir Parquet ou Avro ?**
ğŸ”¹ **Utilise Parquet** pour les **analyses de grandes bases** de donnÃ©es.  
ğŸ”¹ **Utilise Avro** pour les **transactions et le streaming**.  

Ces formats sont essentiels pour **optimiser le stockage et les performances des donnÃ©es** .

